## Abstract
In this paper, we present an efficient and robust deep learning solution for novel view synthesis of complex scenes. In our approach, a 3D scene is represented as a light field, i.e., a set of rays, each of which has a correspond- ing color when reaching the image plane. For efficient novel view rendering, we adopt a two-plane parameterization of the light field, where each ray is characterized by a 4D pa- rameter. We then formulate the light field as a 4D function that maps 4D coordinates to corresponding color values. We train a deep fully connected network to optimize this im- plicit function and memorize the 3D scene. Then, the scene- specific model is used to synthesize novel views. Different from previous light field approaches which require dense view sampling to reliably render novel views, our method can render novel views by sampling rays and querying the color for each ray from the network directly, thus enabling high-quality light field rendering with a sparser set of train- ing images. Per-ray depth can be optionally predicted by the network, thus enabling applications such as auto refo- cus. Our novel view synthesis results are comparable to the state-of-the-arts, and even superior in some challenging scenes with refraction and reflection. We achieve this while maintaining an interactive frame rate and a small memory footprint.

## 1. Introduction
Novel view synthesis has long been studied by the com- puter vision and computer graphics community. It has many applications in multimedia, AR/VR, gaming, etc. Tra- ditional computer vision approaches such as multi-view stereo (MVS) and structure-from-motion (SfM) aim to build a geometric representation of the scene first. An alternative approach is image-based rendering [2, 8, 16], where no un- derlying geometric model or only a simple proxy is needed. These methods can achieve photorealistic rendering. How- ever, a typical light field setup prefers a dense sampling of views around a scene. It thus limits practical use of such an approach.

With the recent advancement of neural rendering [39], photorealistic rendering with only a sparse set of inputs can be achieved. One approach is to use an explicit geomet- ric representation of a scene reconstructed using a tradi- tional computer vision pipeline and learning-based render-ing. Object-specific or category-specific meshes or multi- plane images (MPI) [48] can be used as the representa- tion. However, these explicit representations do not allow a network to learn the optimal representation of the scene. To achieve this, volume-based representations can be used [20]. But they typically require a large amount of memory space, especially for complex scenes.

Memory-efficient implicit representations have gained interests from the research community. For example, surface-based implicit representations can achieve state-of- the-art results and can provide a high-quality reconstruc- tion of the scene geometry [14]. However, surface-based representations face challenges when dealing with complex lighting and geometry, such as transparency, translucency, and thin geometric structures. More recently, volume-based implicit representation achieves remarkable rendering re- sults (e.g., NeRF [25]) and inspires follow-up research. One drawback of NeRF, nevertheless, is the time complexity of rendering. NeRF, in its original form, needs to query the network multiple times per ray and accumulate color and density along the query ray, which prohibits real-time appli- cations. Although there have been many efforts to acceler- ate NeRF, they typically require depth proxy to train or rely on additional storage to achieve faster rendering [11,28,45].

We propose an efficient novel view synthesis framework, which we call Neural 4D Light Field (NeuLF). We define a scene as an implicit function that maps 4D light field rays to corresponding color values directly, This function can be implemented as a Multilayer Perceptron (MLP) and can be learned using only a sparse set of calibrated images placed in front of the scene. This formulation allows the color of a camera ray to be learned directly by the network and does not require a time-consuming ray-marcher during ren- dering as in NeRF. Thus, NeuLF achieves 1000x speedup over NeRF during inference, while producing similar or even better rendering quality. Our light field setup limits the novel viewpoints to be on the same side of the cam- eras, e.g., front views only. Despite these constraints, we argue that for many applications such as teleconferencing, these are reasonable trade-offs to gain much faster inference speed with high-quality rendering and a small memory foot- print.

Comparison with NeRF: Although our work is inspired by NeRF [25], there are some key distinctions. NeRF repre- sents the continuous scene function as a 5D radiance field. Such a representation has redundancy, i.e., color along a ray is constant in free space. By restricting the novel view- points to be outside of the convex hull of the object, the 5D radiance field can be reduced to a light field in a lower di- mension, e.g. 4D. Table 1, we summarize the differences between NeuLF and NeRF.

Moreover, NeuLF can also optionally estimate per ray depth by enforcing multi-view and depth consistency. Using depth, applications such as auto refocus can be enabled. We show state-of-the-art novel view synthesis results on benchmark datasets and our own captured data (Fig. 1). The comparisons with existing approaches also validate the ef- ficiency and effectiveness of our proposed method. In sum- mary, our contributions are:
- We proposed a fast and memory-efficient novel view synthesis pipeline, which solves the mapping from 4D rays to colors directly.

- Comparedwiththestate-of-the-arts,ourmethodisbet- ter than NeRF and NeX when the scene contains chal- lenging refraction and reflection effects. In addition, our method only needs 25% of the original input on those challenge scenes to achieve a similar or even bet- ter quality.

- Application wise, the framework we proposed can op- tionally estimate depth per ray; thus enabling applica- tions such as 3D reconstruction and auto refocus.

## 2. Related Work
Our work builds upon previous work in traditional image-based rendering and implicit-function-based neural scene representation. In the following sections, we will re- view these fields and beyond in detail.

Image-based Rendering For novel view synthesis, image-based rendering has been studied as an alternative to geometric methods. In the seminal work of light field rendering [16], a 5D radiance field is reduced to a 4D light field considering the radiance along a ray remains constant in free space. The ray set in a light field can be parame- terized in different ways, among which two-plane parame- terization is the most common one. Rendering novel views from the light field involves extracting corresponding 2D slices from the 4D light field. To achieve better view in- terpolation, approximate geometry can be used [2, 8, 42]. Visual effects of variable focus and variable depth-of-field can also be achieved using light field [12].

With the advancement of deep learning, a few learning- based methods have been proposed to improve the tradi- tional light field. For example, LFGAN [4] can learn texture and geometry information from light field data sets and in turn predict a small light field from one RGB image. [23] enables high-quality reconstruction of a light field by learn- ing the geometric features hierarchically using a residual network. [43] integrates an anti-aliasing module in a net- work to reduce the artifacts in the reconstructed light field. Our method learns an implicit function of the light field and achieves high-quality reconstruction with a sparse input.

Neural Scene Representation Neural rendering is an emerging field. One of the most important applications of neural rendering is novel view synthesis. A comprehensive survey of the topic can be found in [39].

An explicit geometric model can be used as the repre- sentation of a scene. [29] creates a proxy geometry of the scene using structure from motion and multi-view stereo (MVS). Then, a recurrent encoder-decoder network is used to synthesize new views from nearby views. To improve blending on imperfect meshes from MVS, [10] uses pre- dicted weights from a network to perform blending. A high- quality parameterized mesh of the human body [47] and category-specific mesh reconstruction [13] can also be used as the proxy. Recently, Multi-plane Image (MPI) [48] has gained popularity. [48] learns to predict MPIs from stereo images. The range of novel views is later improved by [37]. [6] uses learned gradient descent to generate an MPI from a set of sparse inputs. [24] uses an MPI representation for turning each sampled view into a local light field. NeX [41] represents each pixel of an MPI with a linear combination of basis functions and achieves state-of-the-art rendering re- sults in real-time. MPI representation might typically lead to stack-of-cards artifacts. [34] trains a network to recon- struct both the geometry and appearance of a scene on a 3D grid. For dynamic scenes, Neural Volumes (NV) [20] uses an encoder-decoder network to convert input images into a 3D volume representation. [21] extends NV using a mixture of volumetric primitives to achieve better and faster render- ing. While volume-based representations allow for learning the 3D structure, they require large memory space, espe- cially for large scenes.

Implicit-function-based approaches provide memory- efficient alternatives to explicit representations, while still allowing learning the 3D structure of the scene. Implicit representations can be categorized as implicit surface-based and implicit volume-based approaches. SRN [35] maps 3D coordinates to a local feature embedding at these coordi- nates. Then, a trained ray-marcher and a pixel generator are used to render novel views. IDR [44] uses an implicit Signed Distance Function (SDF) to model an object on 3D surface reconstruction. Neural Lumigraph [14] provides even better rendering quality by utilizing a sinusoidal rep- resentation network (SIREN) to model the SDF.

Our work is inspired by NeRF [25], which uses a net- work to map continuous 5D coordinates (location and view direction) to volume density and view-dependent radiance. Recent works have extended NeRF to support novel illumi- nation conditions [36], rendering from unstructured image collections from the internet [22], large-scale unbounded scenes [46], unknown camera parameters [40], anti-aliasing [1], deformable models [27], dynamic scenes [17], etc. A lot of effort has been put into speeding up rendering with NeRF. DONeRF [26] places samples around scene surfaces by predicting sample locations along each ray. However, transparent objects will pose issues and it requires ground- truth depth for training. FastNeRF [7] achieves 200fps by factoring NeRF into a position-dependent network and a view-dependent network. This allows efficient caching of network outputs during rendering. [45] trains a NeRF-SH network, which maps coordinates to spherical harmonic co- efficients and pre-samples the NeRF-SH into a sparse voxel- based octree structure. These pre-sampling approaches sac- rifice additional memory storage for speedups. NSVF [19] represents a scene using a set of NeRF-like implicit fields defined on voxels and uses a sparse octree to achieve 10x speedup over NeRF during rendering. KiloNeRF [28] de- composes a scene into a grid of voxels and uses a smaller NeRF for each voxel. Storage costs will increase when more networks are used. Using AutoInt [18], calculations of any definite integral can be done in two network evalu- ations; this achieves 10x acceleration, but rendering qual- ity is decreased. Compared with these approaches, our method achieves 1000x speedup over NeRF by represent- ing the scene with an implicit 4D light field without any additional pre-sampling or storage overhead.

Recently a concurrent work [33] propose to use a net- work to direct regress the mapping from the 6D Plu ̈cker coordinates to colors. It leverages meta-learning to en- ables view synthesis using a single image observation in ShapeNet dataset [3]. In contrast, we use 4D representation and conduct extensive experiments on real-world scenes. Another concurrent work [5] transforms 4D light field rep- resentation by leveraging Gegenbauer polynomials basis, and learning the mapping from this basis function to color, however, it requires dense narrow baseline input with planar camera arrangement.

## 3. Our Method
In Fig. 2, we illustrate the pipeline of our system. In the following sections, we will first briefly discuss the light field, followed by our NeuLF representation and the pro- posed loss functions. We will also discuss our training strategies.
### 3.1. 4D Light Field Representation
All possible light rays in a space can be described by a 5D plenoptic function. Since radiance along a ray is con- stant if viewed from outside of the convex hull of the scene, this 5D function can be reduced to a 4D light field [8, 16]. The most common parameterization is a two-plane model shown in Fig. 3. Each ray from the camera to the scene will intersect with the two planes. Thus, we can represent each ray using the coordinates of the intersections, (u,v) and (s, t), or simply a 4D coordinate (u, v, s, t). Using this representation, all rays from the object to one side of the two planes can be uniquely determined by a 4D coordinate.

Based on this representation, rendering a novel view can be done by querying all the rays from the center of projection to every pixel on the camera’s image plane. We denote them as {R ,R ,...,R }, where N is the total number of 12N pixels. Then, for the i-th ray Ri, we can obtain its 4D co- ordinate (ui , vi , si , ti ) by computing its intersections with the two planes. If a function f maps the continuous 4D coordinates to color values, we can obtain the color of Ri by evaluating the function fc(ui, vi, si, ti). In the next sec- tion, we will introduce Neural 4D Light Field (NeuLF) for reconstructing this mapping function f.

### 3.2. Neural 4D Light Field Reconstruction

We formulate the mapping function fc as a Multilayer Perceptron (MLP). The input of this MLP is a 4D coordi- nate and the output is RGB color. As shown in Fig. 2. The goal of the network is to learn the mapping function from training data.
###### Training Data: 
for a given scene, the training data comes from a set of captured images {I1, I2, ..., IM }, where M is the total number of images. Assuming the camera pose for each image is known or obtainable, for each image Ik (k = 1, ..., M ), we can traverse its pixels and generate all corresponding rays {R1k , R2k , ..., RNk }, where Nk is the total number of pixels in the k-th image. Based on the 4D light field representation, all 4D coordinates {uk1,v1k,sk1,tk1},...,{ukN ,vNk ,skN ,tkN} ,(k = kkkk 1, ..., M ), can be obtained. On the other hand, the color for each pixel is known from the input images. To this end, we have constructed a collection of sample mappings from 4D coordinates to color values uki , vik, ski , tki → cki , (k = 1...M,i = 1...Nk), where cki is the color of the i-th pixel on the k-th image. By feeding this training data to the MLP network, the parameters Θ can be learned by minimizing the following photometric loss Lp:
![[Pasted image 20240109223659.png]]
In Fig. 2, we demonstrate an example of capturing im- ages to train our neural 4D light field representation with a set of unstructured front-faced camera views. In this exam- ple, the cameras are placed on one side of two light slabs.

###### Rendering: 
Given a viewpoint V, we can render a novel view R(V) by evaluating the learned mapping function f. With the camera pose and the desired rendering resolution {W V, HV}, we sample all rays {R1V, R2V, ..., RNV }, where V NV =WV×HVisthenumberofpixelstoberendered.We